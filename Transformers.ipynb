{"cells":[{"cell_type":"markdown","metadata":{"id":"MygJmkQ9guFs"},"source":["# Text classification with Transformer\n","\n","**Description:** Implement a Transformer block as a Keras layer and use it for text classification."]},{"cell_type":"markdown","metadata":{"id":"-dZDdMzsguFy"},"source":["## Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"o8j595GQguF5","executionInfo":{"status":"ok","timestamp":1712405407724,"user_tz":240,"elapsed":7116,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers"]},{"cell_type":"markdown","metadata":{"id":"4O7rPnWlguF7"},"source":["## Implement a Transformer block as a layer"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Nf4LjpxTguF7","executionInfo":{"status":"ok","timestamp":1712405669480,"user_tz":240,"elapsed":142,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}}},"outputs":[],"source":["class TransformerBlock(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super().__init__()\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6) #\"epsilon\" means small float added to variance to avoid dividing by zero. Defaults to 1e-3.\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)"]},{"cell_type":"markdown","metadata":{"id":"d5iXyzSVguF8"},"source":["## Implement embedding layer\n","\n","Two seperate embedding layers, one for tokens, one for token index (positions)."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"DWSPEJAWguF8","executionInfo":{"status":"ok","timestamp":1712405964201,"user_tz":240,"elapsed":589,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}}},"outputs":[],"source":["class TokenAndPositionEmbedding(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super().__init__()\n","        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x)\n","        return x + positions"]},{"cell_type":"markdown","metadata":{"id":"OFpcVWTQguF9"},"source":["## Download and prepare dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"LukPvp93guF-","executionInfo":{"status":"ok","timestamp":1712405974361,"user_tz":240,"elapsed":4300,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"9c046348-a29c-42cc-8f73-d5eaa9d3c256","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17464789/17464789 [==============================] - 0s 0us/step\n","25000 Training sequences\n","25000 Validation sequences\n"]}],"source":["vocab_size = 20000  # Only consider the top 20k words\n","maxlen = 200  # Only consider the first 200 words of each movie review\n","(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n","print(len(x_train), \"Training sequences\")\n","print(len(x_val), \"Validation sequences\")\n","x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n","x_val = keras.utils.pad_sequences(x_val, maxlen=maxlen)"]},{"cell_type":"code","source":["x_train"],"metadata":{"id":"giGUR9dgsW0-","executionInfo":{"status":"ok","timestamp":1712405985321,"user_tz":240,"elapsed":164,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"7a9e87f3-2f83-4b48-d49c-9fe90028775c","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[   5,   25,  100, ...,   19,  178,   32],\n","       [   0,    0,    0, ...,   16,  145,   95],\n","       [   0,    0,    0, ...,    7,  129,  113],\n","       ...,\n","       [   0,    0,    0, ...,    4, 3586,    2],\n","       [   0,    0,    0, ...,   12,    9,   23],\n","       [   0,    0,    0, ...,  204,  131,    9]], dtype=int32)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["y_train"],"metadata":{"id":"BwqVeHmmthgk","executionInfo":{"status":"ok","timestamp":1712405987274,"user_tz":240,"elapsed":3,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"f861a46a-b4d0-4ea4-fd21-4033ab768dc9","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 0, 0, ..., 0, 1, 0])"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"h3UFY6HjguF-"},"source":["## Create classifier model using transformer layer\n","\n","Transformer layer outputs one vector for each time step of our input sequence.\n","Here, we take the mean across all time steps and\n","use a feed forward network on top of it to classify text."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"LjmZMiiPguF-","executionInfo":{"status":"ok","timestamp":1712406083249,"user_tz":240,"elapsed":1343,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}}},"outputs":[],"source":["embed_dim = 32  # Embedding size for each token\n","num_heads = 2  # Number of attention heads\n","ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n","\n","inputs = layers.Input(shape=(maxlen,))\n","embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n","x = embedding_layer(inputs)\n","transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n","x = transformer_block(x)\n","x = layers.GlobalAveragePooling1D()(x)\n","x = layers.Dropout(0.1)(x)\n","x = layers.Dense(20, activation=\"relu\")(x)\n","x = layers.Dropout(0.1)(x)\n","outputs = layers.Dense(2, activation=\"softmax\")(x)\n","\n","model = keras.Model(inputs=inputs, outputs=outputs)"]},{"cell_type":"markdown","metadata":{"id":"777n_tR2guF_"},"source":["## Train and Evaluate"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"tomdP9QHguF_","executionInfo":{"status":"ok","timestamp":1712406232357,"user_tz":240,"elapsed":144865,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"5ca5afa4-3606-4bb8-ff5c-2324abe0638b","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","782/782 [==============================] - 74s 85ms/step - loss: 0.3866 - accuracy: 0.8155 - val_loss: 0.2979 - val_accuracy: 0.8700\n","Epoch 2/2\n","782/782 [==============================] - 21s 27ms/step - loss: 0.2012 - accuracy: 0.9234 - val_loss: 0.3126 - val_accuracy: 0.8713\n"]}],"source":["model.compile(\n","    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",")\n","history = model.fit(\n","    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n",")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/nlp/ipynb/text_classification_with_transformer.ipynb","timestamp":1699192168989}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"}},"nbformat":4,"nbformat_minor":0}